import sqlite3
import numpy as np
import os
import openai
import tiktoken  # biblioteka do pracy z tokenami
from tqdm import tqdm  # ğŸ”¹ Pasek postÄ™pu
from dotenv import load_dotenv

# ğŸ”¹ ZaÅ‚aduj zmienne Å›rodowiskowe z pliku .env
load_dotenv()

# --- KONFIGURACJA ---
TXT_DIR = "./downloaded_pdfs"            # Katalog z plikami TXT
CHUNK_TOKENS = 1000                      # Rozmiar chunku w tokenach
CHUNK_OVERLAP = 50                       # Ile tokenÃ³w ma nachodziÄ‡ na kolejny chunk
OPENAI_MODEL = "text-embedding-ada-002"  # Model OpenAI do embeddowania

# ğŸ”¹ Pobierz klucz API i inicjalizuj klienta OpenAI (dla wersji 1.0.0+)
api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=api_key)

# --- Funkcja do dzielenia tekstu na fragmenty wedÅ‚ug tokenÃ³w ---
def chunk_text_by_tokens(text, tokens_per_chunk=CHUNK_TOKENS, overlap=CHUNK_OVERLAP, model_name=OPENAI_MODEL):
    """
    Dzieli tekst na fragmenty o dÅ‚ugoÅ›ci okreÅ›lonej liczbÄ… tokenÃ³w (tokens_per_chunk).
    Dodaje overlap (wspÃ³lnÄ… liczbÄ™ tokenÃ³w) miÄ™dzy kolejnymi fragmentami.
    """
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    tokens = encoding.encode(text)  # Zamieniamy caÅ‚y tekst na listÄ™ tokenÃ³w
    chunks = []
    start = 0

    while start < len(tokens):
        end = start + tokens_per_chunk
        chunk_slice = tokens[start:end]
        chunk_text = encoding.decode(chunk_slice)
        chunks.append(chunk_text)
        start += tokens_per_chunk - overlap  # PrzesuniÄ™cie z overlapem

    return chunks

# --- PoÅ‚Ä…czenie z bazÄ… danych ---
conn = sqlite3.connect("vectors.db")
cursor = conn.cursor()

# --- Tworzenie tabeli (jeÅ›li nie istnieje) ---
cursor.execute("""
CREATE TABLE IF NOT EXISTS document_vectors (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT,
    chunk_index INTEGER,
    text_snippet TEXT,
    vector BLOB
)
""")

# --- Automatyczne pobieranie plikÃ³w TXT ---
files = [f for f in os.listdir(TXT_DIR) if f.endswith(".txt")]

if not files:
    print("âš ï¸ Brak plikÃ³w TXT w katalogu!")
    exit(0)

print(f"ğŸ” Znaleziono {len(files)} plikÃ³w TXT do wektoryzacji.")

# --- GÅ‚Ã³wna pÄ™tla: odczyt plikÃ³w, dzielenie, generowanie embeddingÃ³w, zapis do DB ---
for file in tqdm(files, desc="ğŸ“‚ Przetwarzanie plikÃ³w"):
    file_path = os.path.join(TXT_DIR, file)
    
    if not os.path.exists(file_path):
        print(f"âš ï¸ Plik nie istnieje: {file_path}")
        continue
    
    # Odczytujemy tekst z pliku
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    if not text:
        print(f"âš ï¸ Plik jest pusty: {file_path}")
        continue

    # Dzielimy tekst wedÅ‚ug tokenÃ³w z overlapem
    chunks = chunk_text_by_tokens(text, CHUNK_TOKENS, CHUNK_OVERLAP, OPENAI_MODEL)

    # Pasek postÄ™pu dla embeddingÃ³w
    for i, chunk in enumerate(tqdm(chunks, desc=f"ğŸ”¹ {file}", leave=False)):
        response = client.embeddings.create(input=chunk, model=OPENAI_MODEL)
        vector = np.array(response.data[0].embedding, dtype=np.float32)
        vector_blob = vector.tobytes()
        
        cursor.execute(
            "INSERT INTO document_vectors (filename, chunk_index, text_snippet, vector) VALUES (?, ?, ?, ?)",
            (file, i, chunk[:200], vector_blob)
        )

conn.commit()
conn.close()

print("âœ… Nowe wektory zapisane do SQLite z wykorzystaniem chunkowania po tokenach i overlapu!")