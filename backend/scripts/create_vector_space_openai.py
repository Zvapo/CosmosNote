import sqlite3
import numpy as np
import os
import openai
import tiktoken  # biblioteka do pracy z tokenami
from dotenv import load_dotenv

# Za≈Çaduj zmienne ≈õrodowiskowe z pliku .env
load_dotenv()

# --- KONFIGURACJA ---
TXT_DIR = "./downloaded_pdfs"            # Katalog z plikami TXT
CHUNK_TOKENS = 1000                      # Rozmiar chunku w tokenach
CHUNK_OVERLAP = 50                       # Ile token√≥w ma nachodziƒá na kolejny chunk
OPENAI_MODEL = "text-embedding-ada-002"  # Model OpenAI do embeddowania

# Pobierz klucz API z zmiennych ≈õrodowiskowych lub wpisz bezpo≈õrednio
openai.api_key = os.getenv("OPENAI_API_KEY")

# --- Funkcja do dzielenia tekstu na fragmenty wed≈Çug token√≥w ---
def chunk_text_by_tokens(text, tokens_per_chunk=CHUNK_TOKENS, overlap=CHUNK_OVERLAP, model_name=OPENAI_MODEL):
    """
    Dzieli tekst na fragmenty o d≈Çugo≈õci okre≈õlonej liczbƒÖ token√≥w (tokens_per_chunk).
    Dodaje overlap (wsp√≥lnƒÖ liczbƒô token√≥w) miƒôdzy kolejnymi fragmentami.
    """
    # Odczytujemy enkoder dopasowany do danego modelu (lub np. 'cl100k_base')
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        # W razie problem√≥w z rozpoznaniem modelu, mo≈ºna u≈ºyƒá domy≈õlnego:
        encoding = tiktoken.get_encoding("cl100k_base")

    # Zamieniamy ca≈Çy tekst na listƒô token√≥w
    tokens = encoding.encode(text)

    chunks = []
    start = 0

    # Dop√≥ki nie osiƒÖgniemy ko≈Ñca listy token√≥w, wycinamy fragment
    while start < len(tokens):
        end = start + tokens_per_chunk
        chunk_slice = tokens[start:end]

        # Dekodujemy tokeny z powrotem do ciƒÖgu tekstowego
        chunk_text = encoding.decode(chunk_slice)
        chunks.append(chunk_text)

        # Przesuwamy "okno" chunku w prawo, ale czƒô≈õƒá (overlap) siƒô nak≈Çada
        start += tokens_per_chunk - overlap

    return chunks

# --- Po≈ÇƒÖczenie z bazƒÖ danych ---
conn = sqlite3.connect("vectors.db")
cursor = conn.cursor()

# --- Tworzenie tabeli (je≈õli nie istnieje) ---
cursor.execute("""
CREATE TABLE IF NOT EXISTS document_vectors (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT,
    chunk_index INTEGER,
    text_snippet TEXT,
    vector BLOB
)
""")

# --- Automatyczne pobieranie plik√≥w TXT ---
files = [f for f in os.listdir(TXT_DIR) if f.endswith(".txt")]

if not files:
    print("‚ö†Ô∏è Brak plik√≥w TXT w katalogu!")
else:
    print(f"üîç Znaleziono {len(files)} plik√≥w TXT do wektoryzacji.")

# --- G≈Ç√≥wna pƒôtla: odczyt plik√≥w, dzielenie, generowanie embedding√≥w, zapis do DB ---
for file in files:
    file_path = os.path.join(TXT_DIR, file)
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è Plik nie istnieje: {file_path}")
        continue  # Pomijamy plik, je≈õli nie istnieje
    
    # Odczytujemy tekst z pliku
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    if not text:  # Je≈õli plik jest pusty, pomijamy
        print(f"‚ö†Ô∏è Plik jest pusty: {file_path}")
        continue

    # Dzielimy tekst wed≈Çug token√≥w z overlapem
    chunks = chunk_text_by_tokens(text, CHUNK_TOKENS, CHUNK_OVERLAP, OPENAI_MODEL)

    # Dla ka≈ºdego chunka ‚Äì generujemy embedding, zapis do bazy
    for i, chunk in enumerate(chunks):
        response = openai.Embedding.create(input=chunk, model=OPENAI_MODEL)
        vector = np.array(response["data"][0]["embedding"], dtype=np.float32)
        vector_blob = vector.tobytes()
        
        # Zachowujemy fragment tekstu (np. do 200 znak√≥w) jako "podglƒÖd"
        cursor.execute(
            "INSERT INTO document_vectors (filename, chunk_index, text_snippet, vector) VALUES (?, ?, ?, ?)",
            (file, i, chunk[:200], vector_blob)
        )

conn.commit()
conn.close()

print("‚úÖ Nowe wektory zapisane do SQLite z wykorzystaniem chunkowania po tokenach i overlapu!")